plot.data0 <- list.df.perf[[n]]
View(plot.data0)
col.dev.fit <- c(colnames(plot.data0)[grepl(".dev.fit", colnames(plot.data0))], "Null_model")
col.dev.pred <- c(colnames(plot.data0)[grepl(".dev.pred", colnames(plot.data0))], "Null_model")
col.auc.fit <- c(colnames(plot.data0)[grepl(".auc.fit", colnames(plot.data0))], "Null_model")
col.auc.pred <- c(colnames(plot.data0)[grepl(".auc.pred", colnames(plot.data0))], "Null_model")
col.dev.pred
plot.data1 <- gather(plot.data0, key = model, value = performance.fit, all_of(col.dev.fit))
plot.data2 <- gather(plot.data0, key = model, value = performance.pred, all_of(col.dev.pred))
plot.data3 <- gather(plot.data0, key = model, value = auc.fit, all_of(col.auc.fit))
plot.data4 <- gather(plot.data0, key = model, value = auc.pred, all_of(col.auc.pred))
plot.data1$model <- sub(".dev.fit", "", plot.data1$model)
View(plot.data1)
plot.data1 <- plot.data1[,c("Taxa", "Prevalence", # "Taxonomic level",
"model", "performance.fit")]
plot.data1 <- gather(plot.data0, key = model, value = performance.fit, all_of(col.dev.fit))
View(plot.data1)
View(plot.data0)
all_of(col.dev.fit)
all_of(col.dev.pred)
plot.data1 <- gather(plot.data0, key = model, value = performance.fit, all_of(col.dev.fit))
plot.data2 <- gather(plot.data0, key = model, value = performance.pred, all_of(col.dev.pred))
plot.data3 <- gather(plot.data0, key = model, value = auc.fit, all_of(col.auc.fit))
plot.data4 <- gather(plot.data0, key = model, value = auc.pred, all_of(col.auc.pred))
plot.data1$model <- sub(".dev.fit", "", plot.data1$model)
plot.data1 <- plot.data1[,c("Taxa", "Prevalence", # "Taxonomic level",
"model", "performance.fit")]
plot.data2$model <- sub(".dev.pred", "", plot.data2$model)
plot.data2 <- plot.data2[,c("Taxa", "Prevalence", # "Taxonomic level",
"model", "performance.pred")]
plot.data3$model <- sub(".auc.fit", "", plot.data3$model)
plot.data3 <- plot.data3[,c("Taxa", "Prevalence", # "Taxonomic level",
"model", "auc.fit")]
plot.data4$model <- sub(".auc.pred", "", plot.data4$model)
plot.data4 <- plot.data4[,c("Taxa", "Prevalence", # "Taxonomic level",
"model", "auc.pred")]
View(plot.data1)
View(plot.data2)
source("utilities.r")
# Prepare data for plots
plot.data <- perf.plot.data(list.df.perf = list.df.perf)
"FIT" %in% names(app.case)
app.case
"FIT" %in% names.app.case
names.appcase <- names(list.df.perf)
"FIT" %in% names.appcase
# Prepare data for plots
plot.data <- perf.plot.data(list.df.perf = list.df.perf)
source("utilities.r")
# Prepare data for plots
plot.data <- perf.plot.data(list.df.perf = list.df.perf)
View(plot.data)
# Boxplots standardized deviance
list.plots <- plot.boxplots.compar.appcase(plot.data = plot.data, list.models = list.models, models.analysis = models.analysis)
models.analysis
list.plots[[1]]
View(plot.data)
app.case <- unique(plot.data$appcase)
app.case
plot.data$appcase == app.case[1]
app.case[1]
source("plot_functions.r")
# Boxplots standardized deviance
list.plots <- plot.boxplots.compar.appcase(plot.data = plot.data, list.models = list.models, models.analysis = models.analysis)
View(plot.data)
source("utilities.r")
# Prepare data for plots
plot.data <- perf.plot.data(list.df.perf = list.df.perf)
source("plot_functions.r")
# Boxplots standardized deviance
list.plots <- plot.boxplots.compar.appcase(plot.data = plot.data, list.models = list.models, models.analysis = models.analysis)
file.name <- "ModelCompar_Boxplots"
if(length(list.df.perf) != 1){
temp.info.file.name <- gsub(
ifelse(CV, "CV_",
ifelse(ODG, paste0(paste(c("ODG_", ODG.info["training.ratio"], ODG.info["variable"]), collapse = ""), "_"),
"FIT_")), "", info.file.name)
} else {
temp.info.file.name <- info.file.name
}
print.pdf.plots(list.plots = list.plots, width = 15, height = ifelse(any(models.analysis == TRUE), 21, 8),
dir.output = dir.plots.output, info.file.name = temp.info.file.name, file.name = file.name,
png = TRUE, png.vertical = ifelse(any(models.analysis == TRUE), T, F), png.ratio = 1.2)
if(!any(models.analysis == T)){
# Standardized deviance vs prevalence
list.plots <- plot.perfvsprev.compar.appcase(plot.data = plot.data, list.models = list.models,
list.taxa = list.taxa)
file.name <- "ModelCompar_PerfVSPrev"
print.pdf.plots(list.plots = list.plots, width = 15, height = ifelse(length(list.df.perf) == 1, 10, 15), dir.output = dir.plots.output, info.file.name = temp.info.file.name, file.name = file.name,
png = TRUE, png.square = TRUE, png.ratio = 0.8)
}
View(plot.data)
summary(data$Region)
summary(as.factor(data$Region))
summary(as.factor(data$BIOGEO))
summary(as.factor(data$RiverBasin))
case.compar
.libPaths()
installed.packages(lib.loc=.libPaths()[1])
getRversion()
?checkpoint
View(data)
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##
## --- A comparison of machine learning and statistical species distribution models: ---
##                -- when overfitting hurts interpretation --
##
##                          --- December 2022 ---
##
## --- Emma Chollet, Andreas Scheidegger, Jonas Wydler and Nele Schuwirth ---
##
##                      --- emma.chollet@eawag.ch ---
##
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# PRELIMINARIES ####
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
getwd() # show working directory, ! set working directory to source file ('main.r') location
rm(list=ls()) # free workspace
graphics.off() # clean graphics display
## Main options ####
BDM <- F # select dataset, "All" or only "BDM" monitoring programs
file.prefix <- ifelse(BDM, "BDM_", "All_")
CV <- T              # train for cross-validation (CV)
ODG <- ifelse(CV, F, # if CV = T, no out-of-domain generalization (ODG)
F  # train for out-of-domain generalization (ODG)
)  # if CV = F and ODG = F, train on whole dataset (FIT)
ODG.info <- c(training.ratio = 0.8,     # ratio of data used for calibration in ODG
variable = "temperature", # factor by which data is split for ODG, can be "random", "temperature", "IAR", etc
model = "lme.area.elev")  # if variable = 'temperature', choose by which one to split, can be "lme.area.elev", "lm.area.elev", etc
train.info <- ifelse(CV, "CV_",
ifelse(ODG, paste0(paste(c("ODG_", ODG.info["training.ratio"], ODG.info["variable"]), collapse = ""), "_"),
"FIT_"))
dl <- F                  # allow data leakage in standardization between training and testing set
if(!CV){ dl <- F }       # if it's only fitting, no dataleakage
models.analysis <- c(    # comparison analysis of different models structures:
"rf.hyperparam"   = F, # RF regularization
"ann.hyperparam"  = F, # ANN hyperparameter tuning
"rf.random"       = F, # sensitivity analysis RF randomness
"ann.random"      = F  # sensitivity analysis ANN randomness
)
case.compar <- c(     # comparison analysis of models trained on different datasets:
"cv.odg"       = ifelse(!CV & !ODG, F, T), # CV vs ODG
"dl"           = F, # data leakage (dl) vs no data leakage (in data standardization)
"temp.model"   = F  # different temperature models
)
server <- F # run the script on the server (changes number of cores)
## Libraries ####
# Set a checkpoint to use same library versions, which makes the code repeatable over time
if ( !require("checkpoint") ) { install.packages("checkpoint"); library("checkpoint") }
checkpoint("2022-01-01", r_version = "4.1.1") # replace with desired date
if ( !require("parallel") ) { install.packages("parallel"); library("parallel") }           # to run things in parallel
# Data management
if ( !require("dplyr") ) { install.packages("dplyr"); library("dplyr") }                    # to sort, join, merge data
if ( !require("tidyr") ) { install.packages("tidyr"); library("tidyr") }                    # to sort, join, merge data
if ( !require("splitTools") ) { install.packages("splitTools"); library("splitTools") }     # to split the data
if ( !require("vtable") ) { install.packages("vtable"); library("vtable") }                 # to make table with summary statistics
if ( !require("pROC") ) { install.packages("pROC"); library("pROC") }                       # to compute AUC
# Plots
if ( !require("ggplot2") ) { install.packages("ggplot2"); library("ggplot2") }              # to do nice plots
if ( !require("gridExtra") ) { install.packages("gridExtra"); library("gridExtra") }        # to arrange multiple plots on a page
if ( !require("plot.matrix") ) { install.packages("plot.matrix"); library("plot.matrix") }  # to plot nice tables
if ( !require("scales") ) { install.packages("scales"); library("scales") }                 # to look at colors
if ( !require("reshape2") ) { install.packages("reshape2"); library("reshape2") }           # to reshape dataframes
if ( !require("DiagrammeR")) { install.packages("DiagrammeR"); library("DiagrammeR") }      # to plot trees of BCT
if ( !require("skimr") ) { install.packages("skimr"); library("skimr") }                    # to show key descriptive stats
if ( !require("corrplot") ) { install.packages("corrplot"); library("corrplot") }           # to plot correlation matrix
if ( !require("gt") ) { install.packages("gt"); library("gt") }                             # to make tables
if ( !require("sf") ) { install.packages("sf"); library("sf") }                             # to read layers for plotting maps
if ( !require("ggpubr") ) { install.packages("ggpubr"); library("ggpubr") }                 # to arrange multiple plots on a page
# Statistical models
if ( !require("rstan") ) { install.packages("rstan"); library("rstan") }                    # to run hierarchical models written in Stan
# Artificial Neural Networks (ANN)
if ( !require("reticulate") ) { install.packages("reticulate"); library("reticulate") }
# install_miniconda()              # run this the very first time reticulate is installed
# install.packages("tensorflow")
library("tensorflow")
# install_tensorflow()             # run this line only when opening new R session
# install.packages("keras")
library("keras")
# install_keras()                  # run this line only when opening new R session
# use_condaenv()
# Machine Learning (ML) models
if ( !require("mgcv") ) { install.packages("mgcv"); library("mgcv") }                       # to run Generalized Additive Model (GAM) algorithm
if ( !require("gam") ) { install.packages("gam"); library("gam") }                          # to run Generalized Additive Model (GAM) algorithm
if ( !require("kernlab") ) { install.packages("kernlab"); library("kernlab") }              # to run Support Vector Machine (SVM) algorithm
if( !require("xgboost") ) { install.packages("xgboost"); library("xgboost") }               # to run Boosted Classification Trees (BCT)
if ( !require("randomForest")) {install.packages("randomForest"); library("randomForest") } # to run Random Forest (RF)
# have to be loaded at the end to not cache function 'train'
if ( !require("caret") ) { install.packages("caret"); library("caret") }                    # comprehensive framework to build machine learning models
## Functions ####
source("stat_model_functions.r")
source("ml_model_functions.r")
source("ann_model_functions.r")
source("plot_functions.r")
source("utilities.r")
## Data ####
# Define directory and files
dir.input.data        <- "../Input_data/"
dir.workspace         <- "../Output_data/Tables/"
dir.models.output     <- "../Output_data/Trained_models/"
dir.plots.output      <- "../Plots/Models_analysis_plots/"
dir.expl.plots.output <- "../Plots/Explorative_plots/"
file.input.data       <- "All_2729samples_9envfact_lme.area.elev_ModelInputs.csv"
file.prev.taxa        <- "All_2729samples_9envfact_lme.area.elev_PrevalenceTaxa.csv"
# Load input datasets
data      <- read.csv(paste0(dir.input.data, file.input.data), header = T, sep = ",", stringsAsFactors = F)
prev.inv  <- read.csv(paste0(dir.input.data, file.prev.taxa), header = T, sep = ",", stringsAsFactors = F)
# Prepare inputs for geographic plots
inputs    <- map.inputs(directory = paste0(dir.input.data,"Swiss.map.gdb"), data = data)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# DATA WRANGLING ####
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Select env. factors ####
# Select temperature of interest
print(colnames(data)[which(grepl("temperature", colnames(data)))])
select.temp <- "lme.area.elev"
# select.temp <- "lm.area.elev"
# data$temperature <- data[,which(colnames(data) == paste0("temperature.", select.temp))]
# Add quadratic terms for temperature and flow velocity
# data$temperature2 <- data$temperature^2
# data$velocity2 <- data$velocity^2
# Select environmental factors
env.fact <- c("Temperature"                        = "temperature",       # Temp
"Flow velocity"                      = "velocity",          # FV
"Riparian agriculture"               = "A10m",              # A10m
"Livestock unit density"             = "cow.density",       # LUD
"Insecticide application rate"       = "IAR",               # IAR
"Urban area"                         = "urban.area",        # Urban
"Forest-river intersection"          = "FRI",               # FRI
"Forest-river intersection buffer"   = "bFRI",              # bFRI
"Width variability"                  = "width.variability") # WV
env.fact.full <- c(env.fact,
"Temperature2" = "temperature2",
"Velocity2"    = "velocity2")
no.env.fact <- length(env.fact)
# Select sites (categorical) information to keep for analysis
vect.info <- c("Region", "RiverBasin", "BIOGEO")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Preprocess data ####
# Select list of taxa
list.taxa <- colnames(data)[which(grepl("Occurrence.", colnames(data)))]
names.taxa <- gsub("Occurrence.", "", list.taxa)
no.taxa <- length(list.taxa)
select.taxa <- c("Occurrence.Gammaridae", "Occurrence.Psychodidae", "Occurrence.Nemouridae") # select taxa for further analysis
list.taxa.int <- prev.inv[which(prev.inv[, "Prevalence"] < 0.75 & prev.inv[,"Prevalence"] > 0.25), # list taxa with intermediate prevalence
"Occurrence.taxa"]
no.taxa.int <- length(list.taxa.int)
cat("\nSummary information of input dataset:\n",
length(unique(data$SampId)), "samples,\n",
length(unique(data$SiteId)), "sites,\n",
length(list.taxa), "taxa.")
print(summary(as.factor(prev.inv[which(prev.inv$Occurrence.taxa %in% list.taxa), "Taxonomic.level"])))
# Write info for files name
info.file.name.data <- paste0(file.prefix,
dim(data)[1], "samples_",
no.env.fact, "envfact_",
select.temp)
info.file.name.models <- paste0(file.prefix,
no.taxa, "taxa_",
train.info,
ifelse(dl, "DL_", "no_DL_"),
select.temp)
cat("\nMain info: ", info.file.name.models, "\n\n")
# Split data
if(CV|ODG){
splits <- split.data(data = data, list.taxa = list.taxa, dir.workspace = dir.workspace,
info.file.name.data = info.file.name.data, select.temp = select.temp,
CV = CV, ODG = ODG, ODG.info = ODG.info, bottom = T)
} else {
splits <- list("Entire dataset" = data)
}
list.splits <- names(splits)
no.splits <- length(list.splits)
n.cores.splits <- ifelse(server, no.splits, 1) # to use one core per split on the server
# Standardize data
stand.norm.data <- standardize.data(data = data, splits = splits, env.fact.full = env.fact.full, dl = dl, CV = CV, ODG = ODG)
standardized.data <- stand.norm.data$standardized.data
standardized.data.factors <- stand.norm.data$standardized.data.factors
normalization.data <- stand.norm.data$normalization.data
remove(stand.norm.data)
no.splits <- length(splits)
plot.data <- data.frame()
list.plots <- list()
if(no.splits == 1){
if(grepl("Split", names(splits))){
plot.data <- bind_rows(splits[[1]][["Training data"]], splits[[1]][["Testing data"]], .id = "Split")
plot.data$Split <- ifelse(plot.data$Split == 1, "Calibration", "Prediction")
} else {
plot.data <- splits[[1]]
plot.data$Split <- names(splits)
}
} else {
for (n in 1:no.splits) {
# n=1
temp.plot.data <- splits[[n]][["Testing data"]]
temp.plot.data$Split <- paste0("Split",n)
plot.data <- bind_rows(plot.data, temp.plot.data)
}
}
plot.data$Split <- as.factor(plot.data$Split)
plot.data <- na.omit(plot.data)
View(plot.data)
for (v in vect.info ) {
plot.data[which(plot.data[,v] == "RHEIN"),v] <- "RHINE"
}
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
##
## --- A comparison of machine learning and statistical species distribution models: ---
##                -- when overfitting hurts interpretation --
##
##                          --- December 2022 ---
##
## --- Emma Chollet, Andreas Scheidegger, Jonas Wydler and Nele Schuwirth ---
##
##                      --- emma.chollet@eawag.ch ---
##
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# PRELIMINARIES ####
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
getwd() # show working directory, ! set working directory to source file ('main.r') location
rm(list=ls()) # free workspace
graphics.off() # clean graphics display
## Main options ####
BDM <- F # select dataset, "All" or only "BDM" monitoring programs
file.prefix <- ifelse(BDM, "BDM_", "All_")
CV <- T              # train for cross-validation (CV)
ODG <- ifelse(CV, F, # if CV = T, no out-of-domain generalization (ODG)
F  # train for out-of-domain generalization (ODG)
)  # if CV = F and ODG = F, train on whole dataset (FIT)
ODG.info <- c(training.ratio = 0.8,     # ratio of data used for calibration in ODG
variable = "temperature", # factor by which data is split for ODG, can be "random", "temperature", "IAR", etc
model = "lme.area.elev")  # if variable = 'temperature', choose by which one to split, can be "lme.area.elev", "lm.area.elev", etc
train.info <- ifelse(CV, "CV_",
ifelse(ODG, paste0(paste(c("ODG_", ODG.info["training.ratio"], ODG.info["variable"]), collapse = ""), "_"),
"FIT_"))
dl <- F                  # allow data leakage in standardization between training and testing set
if(!CV){ dl <- F }       # if it's only fitting, no dataleakage
models.analysis <- c(    # comparison analysis of different models structures:
"rf.hyperparam"   = F, # RF regularization
"ann.hyperparam"  = F, # ANN hyperparameter tuning
"rf.random"       = F, # sensitivity analysis RF randomness
"ann.random"      = F  # sensitivity analysis ANN randomness
)
case.compar <- c(     # comparison analysis of models trained on different datasets:
"cv.odg"       = ifelse(!CV & !ODG, F, T), # CV vs ODG
"dl"           = F, # data leakage (dl) vs no data leakage (in data standardization)
"temp.model"   = F  # different temperature models
)
server <- F # run the script on the server (changes number of cores)
## Libraries ####
# Set a checkpoint to use same library versions, which makes the code repeatable over time
if ( !require("checkpoint") ) { install.packages("checkpoint"); library("checkpoint") }
checkpoint("2022-01-01", r_version = "4.1.1") # replace with desired date
if ( !require("parallel") ) { install.packages("parallel"); library("parallel") }           # to run things in parallel
# Data management
if ( !require("dplyr") ) { install.packages("dplyr"); library("dplyr") }                    # to sort, join, merge data
if ( !require("tidyr") ) { install.packages("tidyr"); library("tidyr") }                    # to sort, join, merge data
if ( !require("splitTools") ) { install.packages("splitTools"); library("splitTools") }     # to split the data
if ( !require("vtable") ) { install.packages("vtable"); library("vtable") }                 # to make table with summary statistics
if ( !require("pROC") ) { install.packages("pROC"); library("pROC") }                       # to compute AUC
# Plots
if ( !require("ggplot2") ) { install.packages("ggplot2"); library("ggplot2") }              # to do nice plots
if ( !require("gridExtra") ) { install.packages("gridExtra"); library("gridExtra") }        # to arrange multiple plots on a page
if ( !require("plot.matrix") ) { install.packages("plot.matrix"); library("plot.matrix") }  # to plot nice tables
if ( !require("scales") ) { install.packages("scales"); library("scales") }                 # to look at colors
if ( !require("reshape2") ) { install.packages("reshape2"); library("reshape2") }           # to reshape dataframes
if ( !require("DiagrammeR")) { install.packages("DiagrammeR"); library("DiagrammeR") }      # to plot trees of BCT
if ( !require("skimr") ) { install.packages("skimr"); library("skimr") }                    # to show key descriptive stats
if ( !require("corrplot") ) { install.packages("corrplot"); library("corrplot") }           # to plot correlation matrix
if ( !require("gt") ) { install.packages("gt"); library("gt") }                             # to make tables
if ( !require("sf") ) { install.packages("sf"); library("sf") }                             # to read layers for plotting maps
if ( !require("ggpubr") ) { install.packages("ggpubr"); library("ggpubr") }                 # to arrange multiple plots on a page
# Statistical models
if ( !require("rstan") ) { install.packages("rstan"); library("rstan") }                    # to run hierarchical models written in Stan
# Artificial Neural Networks (ANN)
if ( !require("reticulate") ) { install.packages("reticulate"); library("reticulate") }
# install_miniconda()              # run this the very first time reticulate is installed
# install.packages("tensorflow")
library("tensorflow")
# install_tensorflow()             # run this line only when opening new R session
# install.packages("keras")
library("keras")
# install_keras()                  # run this line only when opening new R session
# use_condaenv()
# Machine Learning (ML) models
if ( !require("mgcv") ) { install.packages("mgcv"); library("mgcv") }                       # to run Generalized Additive Model (GAM) algorithm
if ( !require("gam") ) { install.packages("gam"); library("gam") }                          # to run Generalized Additive Model (GAM) algorithm
if ( !require("kernlab") ) { install.packages("kernlab"); library("kernlab") }              # to run Support Vector Machine (SVM) algorithm
if( !require("xgboost") ) { install.packages("xgboost"); library("xgboost") }               # to run Boosted Classification Trees (BCT)
if ( !require("randomForest")) {install.packages("randomForest"); library("randomForest") } # to run Random Forest (RF)
# have to be loaded at the end to not cache function 'train'
if ( !require("caret") ) { install.packages("caret"); library("caret") }                    # comprehensive framework to build machine learning models
## Functions ####
source("stat_model_functions.r")
source("ml_model_functions.r")
source("ann_model_functions.r")
source("plot_functions.r")
source("utilities.r")
## Data ####
# Define directory and files
dir.input.data        <- "../Input_data/"
dir.workspace         <- "../Output_data/Tables/"
dir.models.output     <- "../Output_data/Trained_models/"
dir.plots.output      <- "../Plots/Models_analysis_plots/"
dir.expl.plots.output <- "../Plots/Explorative_plots/"
file.input.data       <- "All_2729samples_9envfact_lme.area.elev_ModelInputs.csv"
file.prev.taxa        <- "All_2729samples_9envfact_lme.area.elev_PrevalenceTaxa.csv"
# Load input datasets
data      <- read.csv(paste0(dir.input.data, file.input.data), header = T, sep = ",", stringsAsFactors = F)
prev.inv  <- read.csv(paste0(dir.input.data, file.prev.taxa), header = T, sep = ",", stringsAsFactors = F)
# Prepare inputs for geographic plots
inputs    <- map.inputs(directory = paste0(dir.input.data,"Swiss.map.gdb"), data = data)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# DATA WRANGLING ####
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Select env. factors ####
# Select temperature of interest
print(colnames(data)[which(grepl("temperature", colnames(data)))])
select.temp <- "lme.area.elev"
# select.temp <- "lm.area.elev"
# data$temperature <- data[,which(colnames(data) == paste0("temperature.", select.temp))]
# Add quadratic terms for temperature and flow velocity
# data$temperature2 <- data$temperature^2
# data$velocity2 <- data$velocity^2
# Select environmental factors
env.fact <- c("Temperature"                        = "temperature",       # Temp
"Flow velocity"                      = "velocity",          # FV
"Riparian agriculture"               = "A10m",              # A10m
"Livestock unit density"             = "cow.density",       # LUD
"Insecticide application rate"       = "IAR",               # IAR
"Urban area"                         = "urban.area",        # Urban
"Forest-river intersection"          = "FRI",               # FRI
"Forest-river intersection buffer"   = "bFRI",              # bFRI
"Width variability"                  = "width.variability") # WV
env.fact.full <- c(env.fact,
"Temperature2" = "temperature2",
"Velocity2"    = "velocity2")
no.env.fact <- length(env.fact)
# Select sites (categorical) information to keep for analysis
vect.info <- c("Region", "RiverBasin", "BIOGEO")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## Preprocess data ####
# Select list of taxa
list.taxa <- colnames(data)[which(grepl("Occurrence.", colnames(data)))]
names.taxa <- gsub("Occurrence.", "", list.taxa)
no.taxa <- length(list.taxa)
select.taxa <- c("Occurrence.Gammaridae", "Occurrence.Psychodidae", "Occurrence.Nemouridae") # select taxa for further analysis
list.taxa.int <- prev.inv[which(prev.inv[, "Prevalence"] < 0.75 & prev.inv[,"Prevalence"] > 0.25), # list taxa with intermediate prevalence
"Occurrence.taxa"]
no.taxa.int <- length(list.taxa.int)
cat("\nSummary information of input dataset:\n",
length(unique(data$SampId)), "samples,\n",
length(unique(data$SiteId)), "sites,\n",
length(list.taxa), "taxa.")
print(summary(as.factor(prev.inv[which(prev.inv$Occurrence.taxa %in% list.taxa), "Taxonomic.level"])))
# Write info for files name
info.file.name.data <- paste0(file.prefix,
dim(data)[1], "samples_",
no.env.fact, "envfact_",
select.temp)
info.file.name.models <- paste0(file.prefix,
no.taxa, "taxa_",
train.info,
ifelse(dl, "DL_", "no_DL_"),
select.temp)
cat("\nMain info: ", info.file.name.models, "\n\n")
# Split data
if(CV|ODG){
splits <- split.data(data = data, list.taxa = list.taxa, dir.workspace = dir.workspace,
info.file.name.data = info.file.name.data, select.temp = select.temp,
CV = CV, ODG = ODG, ODG.info = ODG.info, bottom = T)
} else {
splits <- list("Entire dataset" = data)
}
list.splits <- names(splits)
no.splits <- length(list.splits)
n.cores.splits <- ifelse(server, no.splits, 1) # to use one core per split on the server
# Standardize data
stand.norm.data <- standardize.data(data = data, splits = splits, env.fact.full = env.fact.full, dl = dl, CV = CV, ODG = ODG)
standardized.data <- stand.norm.data$standardized.data
standardized.data.factors <- stand.norm.data$standardized.data.factors
normalization.data <- stand.norm.data$normalization.data
remove(stand.norm.data)
# Produce correlation matrix
file.name <- paste0(dir.expl.plots.output, info.file.name.data, "_CorrelationMatrix")
if(!file.exists(paste0(file.name,".pdf"))){
pdf.corr.mat.env.fact(data = data, env.fact = env.fact, file.name = file.name)
}
# Analyze splits
file.name <- paste0("_AnalysisSplits_", train.info, ifelse(ODG, paste0("_by", ODG.info["model"],""), ""))
if(!file.exists(paste0(dir.expl.plots.output, info.file.name.data, file.name, ".pdf"))){
list.plots <- analysis.splits(inputs = inputs, splits = splits, env.fact = env.fact, vect.info = vect.info)
cat("Printing PDF\n")
print.pdf.plots(list.plots = list.plots, width = 15, height = 8, dir.output = dir.expl.plots.output, info.file.name = info.file.name.data, file.name = file.name, png = T)
}
# Analyze splits
file.name <- paste0("_AnalysisSplits_", train.info, ifelse(ODG, paste0("_by", ODG.info["model"],""), ""))
if(!file.exists(paste0(dir.expl.plots.output, info.file.name.data, file.name, ".pdf"))){
list.plots <- analysis.splits(inputs = inputs, splits = splits, env.fact = env.fact, vect.info = vect.info)
cat("Printing PDF\n")
print.pdf.plots(list.plots = list.plots, width = 15, height = 8, dir.output = dir.expl.plots.output, info.file.name = info.file.name.data, file.name = file.name, png = T)
}
dev.off()
# Analyze splits
file.name <- paste0("_AnalysisSplits_", train.info, ifelse(ODG, paste0("_by", ODG.info["model"],""), ""))
if(!file.exists(paste0(dir.expl.plots.output, info.file.name.data, file.name, ".pdf"))){
list.plots <- analysis.splits(inputs = inputs, splits = splits, env.fact = env.fact, vect.info = vect.info)
cat("Printing PDF\n")
print.pdf.plots(list.plots = list.plots, width = 15, height = 8, dir.output = dir.expl.plots.output, info.file.name = info.file.name.data, file.name = file.name, png = T, png.ratio = 0.8)
}
